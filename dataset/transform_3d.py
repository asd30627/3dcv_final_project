import os
import torch
import numpy as np
from numpy import random
import mmcv
from PIL import Image
import math
from copy import deepcopy

from . import OPENOCC_TRANSFORMS


@OPENOCC_TRANSFORMS.register_module()
class DefaultFormatBundle(object):
    """Default formatting bundle.

    It simplifies the pipeline of formatting common fields, including "img",
    "proposals", "gt_bboxes", "gt_labels", "gt_masks" and "gt_semantic_seg".
    These fields are formatted as follows.

    - img: (1)transpose, (2)to tensor, (3)to DataContainer (stack=True)
    - proposals: (1)to tensor, (2)to DataContainer
    - gt_bboxes: (1)to tensor, (2)to DataContainer
    - gt_bboxes_ignore: (1)to tensor, (2)to DataContainer
    - gt_labels: (1)to tensor, (2)to DataContainer
    - gt_masks: (1)to tensor, (2)to DataContainer (cpu_only=True)
    - gt_semantic_seg: (1)unsqueeze dim-0 (2)to tensor,
                       (3)to DataContainer (stack=True)
    """

    def __init__(self, ):
        return

    def __call__(self, results):
        """Call function to transform and format common fields in results.

        Args:
            results (dict): Result dict contains the data to convert.

        Returns:
            dict: The result dict contains the data that is formatted with
                default bundle.
        """
        if 'img' in results:
            if isinstance(results['img'], list):
                # process multiple imgs in single frame
                imgs = [img.transpose(2, 0, 1) for img in results['img']]
                imgs = np.ascontiguousarray(np.stack(imgs, axis=0))
            else:
                imgs = np.ascontiguousarray(results['img'].transpose(2, 0, 1))
            results['img'] = torch.from_numpy(imgs)
        for k in ["teacher_bev_prob", "teacher_occ_prob"]:
            if k in results and isinstance(results[k], np.ndarray):
                results[k] = torch.from_numpy(results[k].astype(np.float32, copy=False))

        return results

    def __repr__(self):
        return self.__class__.__name__


@OPENOCC_TRANSFORMS.register_module()
class NuScenesAdaptor(object):
    def __init__(self, num_cams, use_ego=False):
        self.num_cams = num_cams
        self.projection_key = 'ego2img' if use_ego else 'lidar2img'

    def __call__(self, input_dict):
        # 1. Projection Matrix
        input_dict["projection_mat"] = torch.from_numpy(
            np.float32(np.stack(input_dict[self.projection_key]))
        )  # (N,4,4)

        # 2. Image WH
        input_dict["image_wh"] = torch.from_numpy(
            np.ascontiguousarray(
                np.array(input_dict["img_shape"], dtype=np.float32)[:, :2][:, ::-1]
            )
        )

        # 3. Image Augmentation Matrix (Route A)
        input_dict["img_aug_matrix"] = torch.from_numpy(
            np.ascontiguousarray(input_dict["img_aug_matrix"], dtype=np.float32)
        )
        
        # 4. âœ… [é—œéµä¿®æ­£] è™•ç† BDA Matrix
        input_dict["bda_mat"] = torch.from_numpy(
            np.array(input_dict["bda_mat"], dtype=np.float32)
        )
        
        # 5. âœ… [é—œéµä¿®æ­£] ç¢ºä¿ gt_depth å­˜åœ¨
        # PointToMultiViewDepth å·²ç¶“ç”Ÿæˆäº† Tensorï¼Œé€™è£¡ç¢ºèªä¸€ä¸‹å³å¯
        if "gt_depth" not in input_dict:
             # å¦‚æœæ²’æœ‰è·‘ depth generationï¼Œçµ¦å€‹ç©ºçš„é¿å…å ±éŒ¯ (é›–ç„¶é€™æ¨£è¨“ç·´æœƒçˆ†)
             # å»ºè­°: å ±éŒ¯æé†’
             pass 
        
        # 6) âœ… æŠŠ ego2lidar / lidar2ego ä¸€èµ·å¸¶å‡ºå»ï¼ˆä¿®æ­£ frame ç”¨ï¼‰
        if "ego2lidar" in input_dict:
            ego2lidar = np.asarray(input_dict["ego2lidar"], dtype=np.float32)  # (4,4)
            input_dict["ego2lidar"] = torch.from_numpy(ego2lidar)
            input_dict["lidar2ego"] = torch.from_numpy(np.linalg.inv(ego2lidar).astype(np.float32))

        # âœ… NEW: pass K/R/t
        if "K" in input_dict:
            input_dict["K"] = torch.from_numpy(np.asarray(input_dict["K"], dtype=np.float32))  # (N,3,3)
        if "R" in input_dict:
            input_dict["R"] = torch.from_numpy(np.asarray(input_dict["R"], dtype=np.float32))  # (N,3,3)
        if "t" in input_dict:
            input_dict["t"] = torch.from_numpy(np.asarray(input_dict["t"], dtype=np.float32))  # (N,3)

        #         # NuScenesAdaptor.__call__ æœ€å¾ŒåŠ 
        # if "teacher_bev_prob" in input_dict:
        #     x = np.asarray(input_dict["teacher_bev_prob"], dtype=np.float32)
        #     input_dict["teacher_bev_prob"] = torch.from_numpy(x)

        # if "teacher_occ_prob" in input_dict:
        #     x = np.asarray(input_dict["teacher_occ_prob"], dtype=np.float32)
        #     input_dict["teacher_occ_prob"] = torch.from_numpy(x)

        return input_dict

@OPENOCC_TRANSFORMS.register_module()
class LoadPointsFromFile(object):
    """
    è®€å–åŸå§‹ NuScenes .bin é»é›²æª”æ¡ˆ
    """
    def __init__(self, coord_type, load_dim, use_dim, pc_range, num_pts):
        self.coord_type = coord_type
        self.load_dim = load_dim
        self.use_dim = use_dim
        self.pc_range = pc_range
        self.num_pts = num_pts

    def __call__(self, results):
        # 1. å–å¾—æª”æ¡ˆè·¯å¾‘
        # NuScenesDataset é€šå¸¸æœƒæä¾› 'pts_filename'ï¼Œæ˜¯ç›¸å°è·¯å¾‘ (samples/LIDAR_TOP/xxx.bin)
        pts_filename = results['pts_filename']
        
        # ç¢ºä¿è·¯å¾‘å®Œæ•´ (å¦‚æœ pts_filename åªæ˜¯ç›¸å°è·¯å¾‘)
        # results['img_filename'][0] é€šå¸¸åŒ…å« data_root çš„å‰ç¶´ï¼Œæˆ‘å€‘å¯ä»¥å€Ÿé‘‘
        # ä½†é€šå¸¸ mmengine çš„ dataset æœƒè™•ç†å¥½ï¼Œæˆ‘å€‘å…ˆå‡è¨­è·¯å¾‘æ˜¯å°çš„
        # å¦‚æœå ±éŒ¯æ‰¾ä¸åˆ°æª”æ¡ˆï¼Œé€šå¸¸æ˜¯å› ç‚º data_root æ²’æ¥ä¸Šå»
        if not os.path.exists(pts_filename):
            # å˜—è©¦æ¥ä¸Š data_root (é€šå¸¸åœ¨ dataset åˆå§‹åŒ–æ™‚å®šç¾©ï¼Œæˆ–è€…åœ¨ img_filename è£¡æ‰¾ç·šç´¢)
            # é€™è£¡åšä¸€å€‹ç°¡å–®çš„ fallbackï¼šå‡è¨­ data_root åœ¨ results å¤–éƒ¨å‚³å…¥æˆ–å¯«æ­»
            # æ¯”è¼ƒç©©çš„åšæ³•æ˜¯ä¾è³´ Dataset é¡åˆ¥æŠŠå®Œæ•´è·¯å¾‘å‚³é€²ä¾†
            # é€™è£¡å‡è¨­ results['pts_filename'] å·²ç¶“æ˜¯å®Œæ•´è·¯å¾‘
            pass

        # 2. è®€å– .bin æª”æ¡ˆ (float32)
        try:
            points = np.fromfile(pts_filename, dtype=np.float32)
        except FileNotFoundError:
            # å®¹éŒ¯è™•ç†ï¼šæœ‰äº› dataset è¨­å®š pts_filename æ˜¯ç›¸å°è·¯å¾‘
            # å˜—è©¦æ‰‹å‹•æ‹¼æ¥ data/nuscenes/
            data_root = "data/nuscenes/" 
            points = np.fromfile(os.path.join(data_root, pts_filename), dtype=np.float32)

        # 3. Reshape (N, 5) -> x, y, z, intensity, ring_index
        points = points.reshape(-1, self.load_dim)
        
        # 4. å–å‰å¹¾ç¶­ (é€šå¸¸å–å‰3æˆ–å‰4)
        points = points[:, :self.use_dim] # (N, 3) or (N, 4)

        # 5. Filter (éæ¿¾ç¯„åœå¤–çš„é»)
        mask = (points[:, 0] > self.pc_range[0]) & (points[:, 0] < self.pc_range[3]) & \
               (points[:, 1] > self.pc_range[1]) & (points[:, 1] < self.pc_range[4]) & \
               (points[:, 2] > self.pc_range[2]) & (points[:, 2] < self.pc_range[5])
        points = points[mask]

        # 6. Sampling (éš¨æ©Ÿæ¡æ¨£åˆ°å›ºå®šé»æ•¸ï¼Œé¿å…é¡¯å­˜çˆ†ç‚¸)
        if points.shape[0] < self.num_pts:
            # é»ä¸å¤ ï¼Œé‡è¤‡è£œé»
            choice = np.random.choice(points.shape[0], self.num_pts, replace=True)
        else:
            # é»å¤ªå¤šï¼Œéš¨æ©Ÿé¸
            choice = np.random.choice(points.shape[0], self.num_pts, replace=False)
        
        points = points[choice]

        # 7. å­˜å…¥ results (ç‚ºäº†ç›¸å®¹ä½ çš„ PointToMultiViewDepthï¼Œé€™è£¡å­˜ anchor_points)
        results['anchor_points'] = points.astype(np.float32)
        
        # ç‚ºäº†ç›¸å®¹ mmdet3d çš„æŸäº›ç¿’æ…£ï¼Œä¹Ÿå¯ä»¥å­˜ 'points'
        # results['points'] = points 

        return results

    def __repr__(self):
        return self.__class__.__name__

@OPENOCC_TRANSFORMS.register_module()
class GlobalRotScaleTrans(object):
    """
    ç”Ÿæˆ BDA (BEV Data Augmentation) åƒæ•¸èˆ‡çŸ©é™£ã€‚
    é€™æœƒå½±éŸ¿ View Transformer çš„æŠ•å½±ï¼Œä»¥åŠ Occupancy GT çš„ç¿»è½‰ã€‚
    """
    def __init__(self,
                 rot_range=[-0.3925, 0.3925], # +/- 22.5åº¦
                 scale_ratio_range=[0.95, 1.05],
                 translation_std=[0, 0, 0],
                 flip_dx_ratio=0.5,
                 flip_dy_ratio=0.5):
        self.rot_range = rot_range
        self.scale_ratio_range = scale_ratio_range
        self.translation_std = translation_std
        self.flip_dx_ratio = flip_dx_ratio
        self.flip_dy_ratio = flip_dy_ratio

    def __call__(self, input_dict):
        # 1. Random Sample Params
        rot_angle = np.random.uniform(*self.rot_range)
        scale_ratio = np.random.uniform(*self.scale_ratio_range)
        flip_dx = np.random.rand() < self.flip_dx_ratio
        flip_dy = np.random.rand() < self.flip_dy_ratio
        
        # 2. Build BDA Matrix (3x3)
        # Rotation
        rot_sin = np.sin(rot_angle)
        rot_cos = np.cos(rot_angle)
        rot_mat = np.array([[rot_cos, -rot_sin, 0], 
                            [rot_sin,  rot_cos, 0], 
                            [0,          0,     1]], dtype=np.float32)
        
        # Scale
        scale_mat = np.array([[scale_ratio, 0, 0], 
                              [0, scale_ratio, 0], 
                              [0, 0, scale_ratio]], dtype=np.float32)
        
        # Flip (X and Y)
        flip_mat = np.eye(3, dtype=np.float32)
        if flip_dx: 
            flip_mat = flip_mat @ np.array([[-1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=np.float32)
        if flip_dy: 
            flip_mat = flip_mat @ np.array([[1, 0, 0], [0, -1, 0], [0, 0, 1]], dtype=np.float32)

        # Combine: Flip @ Scale @ Rot
        bda_mat = flip_mat @ (scale_mat @ rot_mat)
        
        # 3. Save to results
        input_dict['bda_mat'] = bda_mat
        input_dict['bda_rot_angle'] = rot_angle
        input_dict['bda_scale'] = scale_ratio
        input_dict['flip_dx'] = flip_dx
        input_dict['flip_dy'] = flip_dy

        # 4. å¦‚æœæœ‰é»é›² (points/anchor_points)ï¼Œä¹Ÿè¦è·Ÿè‘—è½‰
        # é€™æ˜¯ç‚ºäº†è®“ Depth Loss è¨ˆç®—æ­£ç¢º (Depth æ˜¯ç”±é»é›²ç”Ÿæˆçš„)
        # if 'anchor_points' in input_dict:
        #     points = input_dict['anchor_points']
        #     # points: (N, 3) or (N, 4)
        #     # apply rotation/scale/flip
        #     points[:, :3] = points[:, :3] @ bda_mat.T
        #     input_dict['anchor_points'] = points

        return input_dict

@OPENOCC_TRANSFORMS.register_module()
class PointToMultiViewDepth(object):
    def __init__(self, grid_config, downsample=16):
        self.downsample = downsample
        self.grid_config = grid_config

    def points2depthmap(self, points, height, width):
        """
        å°‡é»é›²æŠ•å½±åˆ°åœ–åƒå¹³é¢ç”Ÿæˆæ·±åº¦åœ–
        points: (N, 3) [u, v, d]
        """
        height, width = height // self.downsample, width // self.downsample
        depth_map = torch.zeros((height, width), dtype=torch.float32)
        
        # åº§æ¨™ç¸®æ”¾
        coor = torch.round(points[:, :2] / self.downsample) 
        depth = points[:, 2]

        # ç¯©é¸åœ¨åœ–åƒç¯„åœå…§çš„é» + æ·±åº¦ç¯„åœå…§çš„é»
        kept1 = (coor[:, 0] >= 0) & (coor[:, 0] < width) & \
                (coor[:, 1] >= 0) & (coor[:, 1] < height) & \
                (depth < self.grid_config['depth'][1]) & \
                (depth >= self.grid_config['depth'][0])
        
        coor, depth = coor[kept1], depth[kept1]
        
        # è™•ç†é‡ç–Šé»ï¼šä¿ç•™æ·±åº¦æœ€å°çš„ (æœ€è¿‘çš„)
        ranks = coor[:, 0] + coor[:, 1] * width
        sort = (ranks + depth / 100.).argsort()
        coor, depth, ranks = coor[sort], depth[sort], ranks[sort]
        
        kept2 = torch.ones(coor.shape[0], device=coor.device, dtype=torch.bool)
        kept2[1:] = (ranks[1:] != ranks[:-1])
        coor, depth = coor[kept2], depth[kept2]
        
        coor = coor.to(torch.long)
        depth_map[coor[:, 1], coor[:, 0]] = depth
        return depth_map

    def __call__(self, results):
        points_lidar = results.get("points_lidar", results["anchor_points"])
        if isinstance(points_lidar, np.ndarray):
            points_lidar = torch.from_numpy(points_lidar).float()
        
        # 2. æº–å‚™æŠ•å½±åƒæ•¸ (éœ€è¦ lidar2img)
        # ä½ çš„ pipeline ä¸­ï¼Œlidar2img é€šå¸¸å­˜æ”¾åœ¨ results['lidar2img'] (List of arrays)
        # é€™äº›çŸ©é™£å·²ç¶“åŒ…å«äº† extrinsic + intrinsic
        
        lidar2imgs = results['lidar2img'] # List of 4x4 numpy arrays (6 cams)
        img_aug_matrices = results.get('img_aug_matrix', None) # List of 4x4 (6 cams)
        
        # åœ–åƒå°ºå¯¸ (å‡è¨­æ‰€æœ‰åœ–åƒå°ºå¯¸ç›¸åŒï¼Œä¸”å·²ç¶“ resize é)
        # results['img_shape'] æ˜¯ä¸€å€‹ list [(H,W), ...]
        H, W = results['img_shape'][0] 

        depth_map_list = []
        
        for i in range(len(lidar2imgs)):
            # A. æŠ•å½±: Lidar -> Image (Pixel)
            # P = K @ R @ t
            l2i = torch.from_numpy(lidar2imgs[i]).float() # (4, 4)
            
            # é½Šæ¬¡åæ¨™æŠ•å½±
            # points: (N, 3) -> (N, 4)
            points_h = torch.cat([points_lidar[:, :3], torch.ones_like(points_lidar[:, :1])], dim=1)
            
            # (4, 4) @ (4, N) -> (4, N) -> (N, 4)
            points_img = (l2i @ points_h.T).T 
            
            # æ­¸ä¸€åŒ– (u, v, d)
            # u = x/z, v = y/z, d = z
            depth = points_img[:, 2]
            mask = depth > 1e-5
            points_img = points_img[mask]
            
            # [u, v, d]
            points_img = torch.cat([
                points_img[:, 0:1] / points_img[:, 2:3], 
                points_img[:, 1:2] / points_img[:, 2:3], 
                points_img[:, 2:3]
            ], dim=1)

            # B. æ‡‰ç”¨åœ–åƒå¢å¼· (Resize/Crop/Rotate)
            # å› ç‚º img å·²ç¶“è¢« ResizeCropFlipImage è®Šæ›éäº†ï¼ŒGT Depth ä¹Ÿè¦å°é½Š
            if img_aug_matrices is not None:
                aug_mat = torch.from_numpy(img_aug_matrices[i]).float() # (4, 4)
                
                # å¢å¼·çŸ©é™£é€šå¸¸æ˜¯ 3x3 (åœ¨ 2D å¹³é¢ä¸Š) æˆ– 4x4
                # u' = A @ u
                # points_img: (N, 3) -> (u, v, d)
                # é€™è£¡è¦å°å¿ƒï¼šaug_mat æ˜¯é‡å° (u, v, 1) çš„ï¼Œd æ‡‰è©²ä¿æŒä¸è®Š
                
                # å»ºæ§‹é½Šæ¬¡ uv: (N, 3) [u, v, 1]
                uv1 = torch.cat([points_img[:, :2], torch.ones_like(points_img[:, :1])], dim=1)
                
                # æ‡‰ç”¨å¢å¼·: (4, 4) @ (N, 4)^T ? ä¸ï¼Œaug_mat é€šå¸¸æ˜¯ 4x4 ä½†åªç”¨å‰ 3x3 ä½œç”¨æ–¼ 2D
                # ä½ çš„ ResizeCropFlipImage è¼¸å‡ºçš„ 4x4 æ˜¯:
                # [R00 R01 0 tx]
                # [R10 R11 0 ty]
                # ...
                
                # æˆ‘å€‘åªå–å‰ 3x3 (2D homography) ä½œç”¨æ–¼ pixel coords
                # ä½†è¦æ³¨æ„ aug_mat 4x4 çš„å®šç¾©ã€‚åœ¨ä½ çš„ä»£ç¢¼ä¸­ï¼š
                # mat4[:2, :2] = ida_mat3[:2, :2]
                # mat4[:2,  3] = ida_mat3[:2,  2] 
                # æ‰€ä»¥æ˜¯æ¨™æº–çš„ projection matrix æ ¼å¼
                
                # [u', v', w']^T = Aug @ [u, v, 0, 1]^T
                # ç°¡åŒ–ç®—æ³•ï¼š
                uv1_aug = (aug_mat[:3, :3] @ torch.cat([uv1[:, :2], torch.ones_like(uv1[:, 2:])], dim=1).T).T
                # æˆ–æ˜¯ç›´æ¥ç”¨ 2D è®Šæ›é‚è¼¯
                
                u_new = uv1[:, 0] * aug_mat[0, 0] + uv1[:, 1] * aug_mat[0, 1] + aug_mat[0, 3]
                v_new = uv1[:, 0] * aug_mat[1, 0] + uv1[:, 1] * aug_mat[1, 1] + aug_mat[1, 3]
                
                points_img[:, 0] = u_new
                points_img[:, 1] = v_new
                # depth (index 2) ä¸è®Š

            # C. ç”Ÿæˆ Depth Map
            depth_map = self.points2depthmap(points_img, H, W)
            depth_map_list.append(depth_map)

        # Stack -> (N_views, H/down, W/down)
        results['gt_depth'] = torch.stack(depth_map_list)
        return results

@OPENOCC_TRANSFORMS.register_module()
class ResizeCropFlipImage(object):
    def __call__(self, results):
        aug_configs = results.get("aug_configs")
        if aug_configs is None:
            # è·¯ç·šBï¼šæ²’åš aug å°±çµ¦ identity
            N = len(results["img"])
            results["img_aug_matrix"] = np.tile(np.eye(4, dtype=np.float32)[None], (N, 1, 1))
            return results

        resize, resize_dims, crop, flip, rotate = aug_configs
        imgs = results["img"]
        N = len(imgs)

        new_imgs = []
        ida_mats = []

        for i in range(N):
            img = Image.fromarray(np.uint8(imgs[i]))
            img, ida_mat3 = self._img_transform(
                img, resize=resize, resize_dims=resize_dims, crop=crop, flip=flip, rotate=rotate
            )

            new_imgs.append(np.array(img).astype(np.float32))

            # âœ… 3x3 -> 4x4 (æŠŠ tx,ty æ”¾åˆ° col=3)
            mat4 = np.eye(4, dtype=np.float32)
            ida_mat3 = ida_mat3.cpu().numpy().astype(np.float32)
            mat4[:2, :2] = ida_mat3[:2, :2]
            mat4[:2,  3] = ida_mat3[:2,  2]
            ida_mats.append(mat4)

            # âœ… è·¯ç·šBï¼šæŠŠ augmentation bake é€² lidar2img/ego2img
            # results["lidar2img"][i] = mat4 @ results["lidar2img"][i]
            # if "ego2img" in results:
            #     results["ego2img"][i] = mat4 @ results["ego2img"][i]

        results["img"] = new_imgs
        results["img_shape"] = [x.shape[:2] for x in new_imgs]  # [(H,W), ...]
        results["img_aug_matrix"] = np.stack(ida_mats, axis=0).astype(np.float32)  # [N,4,4]
        return results


    def _get_rot(self, h):
        return torch.Tensor(
            [
                [np.cos(h), np.sin(h)],
                [-np.sin(h), np.cos(h)],
            ]
        )

    def _img_transform(self, img, resize, resize_dims, crop, flip, rotate):
        ida_rot = torch.eye(2)
        ida_tran = torch.zeros(2)

        # --- 0) å…ˆè¨˜éŒ„åŸåœ–å°ºå¯¸ï¼ˆPIL: W,Hï¼‰
        oldW, oldH = img.size

        # --- 1) resize
        # resize_dims æ˜¯ (newW, newH)
        newW, newH = resize_dims
        img = img.resize(resize_dims)

        # âœ… ç”¨ sx, syï¼ˆä¸è¦ç”¨å–®ä¸€ resizeï¼‰
        sx = float(newW) / float(oldW)
        sy = float(newH) / float(oldH)
        S = torch.tensor([[sx, 0.0], [0.0, sy]], dtype=torch.float32)
        ida_rot = S.matmul(ida_rot)

        # --- 2) crop
        img = img.crop(crop)
        ida_tran -= torch.tensor(crop[:2], dtype=torch.float32)

        # --- 3) flip
        if flip:
            img = img.transpose(method=Image.FLIP_LEFT_RIGHT)
            A = torch.tensor([[-1.0, 0.0], [0.0, 1.0]], dtype=torch.float32)
            b = torch.tensor([crop[2] - crop[0], 0.0], dtype=torch.float32)
            ida_rot = A.matmul(ida_rot)
            ida_tran = A.matmul(ida_tran) + b

        # --- 4) rotate (around center, expand=False)
        img = img.rotate(rotate)
        A = self._get_rot(rotate / 180 * np.pi).float()
        b = torch.tensor([crop[2] - crop[0], crop[3] - crop[1]], dtype=torch.float32) / 2
        b = A.matmul(-b) + b
        ida_rot = A.matmul(ida_rot)
        ida_tran = A.matmul(ida_tran) + b

        # --- 5) build 3x3
        ida_mat = torch.eye(3)
        ida_mat[:2, :2] = ida_rot
        ida_mat[:2, 2] = ida_tran
        return img, ida_mat



@OPENOCC_TRANSFORMS.register_module()
class NormalizeMultiviewImage(object):
    """Normalize the image.
    Added key is "img_norm_cfg".
    Args:
        mean (sequence): Mean values of 3 channels.
        std (sequence): Std values of 3 channels.
        to_rgb (bool): Whether to convert the image from BGR to RGB,
            default is true.
    """

    def __init__(self, mean, std, to_rgb=True):
        self.mean = np.array(mean, dtype=np.float32)
        self.std = np.array(std, dtype=np.float32)
        self.to_rgb = to_rgb

    def __call__(self, results):
        """Call function to normalize images.
        Args:
            results (dict): Result dict from loading pipeline.
        Returns:
            dict: Normalized results, 'img_norm_cfg' key is added into
                result dict.
        """
        results["img"] = [
            mmcv.imnormalize(img, self.mean, self.std, self.to_rgb)
            for img in results["img"]
        ]
        results["img_norm_cfg"] = dict(
            mean=self.mean, std=self.std, to_rgb=self.to_rgb
        )
        return results

    def __repr__(self):
        repr_str = self.__class__.__name__
        repr_str += f"(mean={self.mean}, std={self.std}, to_rgb={self.to_rgb})"
        return repr_str


@OPENOCC_TRANSFORMS.register_module()
class PhotoMetricDistortionMultiViewImage:
    """Apply photometric distortion to image sequentially, every transformation
    is applied with a probability of 0.5. The position of random contrast is in
    second or second to last.
    1. random brightness
    2. random contrast (mode 0)
    3. convert color from BGR to HSV
    4. random saturation
    5. random hue
    6. convert color from HSV to BGR
    7. random contrast (mode 1)
    8. randomly swap channels
    Args:
        brightness_delta (int): delta of brightness.
        contrast_range (tuple): range of contrast.
        saturation_range (tuple): range of saturation.
        hue_delta (int): delta of hue.
    """

    def __init__(
        self,
        brightness_delta=32,
        contrast_range=(0.5, 1.5),
        saturation_range=(0.5, 1.5),
        hue_delta=18,
    ):
        self.brightness_delta = brightness_delta
        self.contrast_lower, self.contrast_upper = contrast_range
        self.saturation_lower, self.saturation_upper = saturation_range
        self.hue_delta = hue_delta

    def __call__(self, results):
        """Call function to perform photometric distortion on images.
        Args:
            results (dict): Result dict from loading pipeline.
        Returns:
            dict: Result dict with images distorted.
        """
        imgs = results["img"]
        new_imgs = []
        for img in imgs:
            assert img.dtype == np.float32, (
                "PhotoMetricDistortion needs the input image of dtype np.float32,"
                ' please set "to_float32=True" in "LoadImageFromFile" pipeline'
            )
            # random brightness
            if random.randint(2):
                delta = random.uniform(
                    -self.brightness_delta, self.brightness_delta
                )
                img += delta

            # mode == 0 --> do random contrast first
            # mode == 1 --> do random contrast last
            mode = random.randint(2)
            if mode == 1:
                if random.randint(2):
                    alpha = random.uniform(
                        self.contrast_lower, self.contrast_upper
                    )
                    img *= alpha

            # convert color from BGR to HSV
            img = mmcv.bgr2hsv(img)

            # random saturation
            if random.randint(2):
                img[..., 1] *= random.uniform(
                    self.saturation_lower, self.saturation_upper
                )

            # random hue
            if random.randint(2):
                img[..., 0] += random.uniform(-self.hue_delta, self.hue_delta)
                img[..., 0][img[..., 0] > 360] -= 360
                img[..., 0][img[..., 0] < 0] += 360

            # convert color from HSV to BGR
            img = mmcv.hsv2bgr(img)

            # random contrast
            if mode == 0:
                if random.randint(2):
                    alpha = random.uniform(
                        self.contrast_lower, self.contrast_upper
                    )
                    img *= alpha

            # randomly swap channels
            if random.randint(2):
                img = img[..., random.permutation(3)]
            new_imgs.append(img)
        results["img"] = new_imgs
        return results

    def __repr__(self):
        repr_str = self.__class__.__name__
        repr_str += f"(\nbrightness_delta={self.brightness_delta},\n"
        repr_str += "contrast_range="
        repr_str += f"{(self.contrast_lower, self.contrast_upper)},\n"
        repr_str += "saturation_range="
        repr_str += f"{(self.saturation_lower, self.saturation_upper)},\n"
        repr_str += f"hue_delta={self.hue_delta})"
        return repr_str


@OPENOCC_TRANSFORMS.register_module()
class LoadMultiViewImageFromFiles(object):
    """Load multi channel images from a list of separate channel files.

    Expects results['img_filename'] to be a list of filenames.

    Args:
        to_float32 (bool, optional): Whether to convert the img to float32.
            Defaults to False.
        color_type (str, optional): Color type of the file.
            Defaults to 'unchanged'.
    """

    def __init__(self, to_float32=False, color_type='unchanged', crop_size=None):
        self.to_float32 = to_float32
        self.color_type = color_type
        self.crop_size = crop_size

    def __call__(self, results):
        """Call function to load multi-view image from files.

        Args:
            results (dict): Result dict containing multi-view image filenames.

        Returns:
            dict: The result dict containing the multi-view image data.
                Added keys and values are described below.

                - filename (str): Multi-view image filenames.
                - img (np.ndarray): Multi-view image arrays.
                - img_shape (tuple[int]): Shape of multi-view image arrays.
                - ori_shape (tuple[int]): Shape of original image arrays.
                - pad_shape (tuple[int]): Shape of padded image arrays.
                - scale_factor (float): Scale factor.
                - img_norm_cfg (dict): Normalization configuration of images.
        """
        filename = results['img_filename']
        # img is of shape (h, w, c, num_views)
        img = np.stack(
            [mmcv.imread(name, self.color_type) for name in filename], axis=-1)
        if self.crop_size is not None:
            img = img[:self.crop_size[0], :self.crop_size[1]]
        if self.to_float32:
            img = img.astype(np.float32)
        results['filename'] = filename
        # unravel to list, see `DefaultFormatBundle` in formatting.py
        # which will transpose each image separately and then stack into array
        results['img'] = [img[..., i] for i in range(img.shape[-1])]
        results['ori_img'] = deepcopy(img)
        results['img_shape'] = img.shape
        results['ori_shape'] = img.shape
        # Set initial values for default meta_keys
        results['pad_shape'] = img.shape
        results['scale_factor'] = 1.0
        num_channels = 1 if len(img.shape) < 3 else img.shape[2]
        results['img_norm_cfg'] = dict(
            mean=np.zeros(num_channels, dtype=np.float32),
            std=np.ones(num_channels, dtype=np.float32),
            to_rgb=False)
        return results

    def __repr__(self):
        """str: Return a string that describes the module."""
        repr_str = self.__class__.__name__
        repr_str += f'(to_float32={self.to_float32}, '
        repr_str += f"color_type='{self.color_type}')"
        return repr_str


@OPENOCC_TRANSFORMS.register_module()
class LoadPointFromFile(object):
    def __init__(self, pc_range, num_pts, use_ego=False, keep_normalized=False):
        self.use_ego = use_ego
        self.pc_range = pc_range
        self.num_pts = num_pts
        self.keep_normalized = keep_normalized  # æƒ³ä¿ç•™ 0~1 å†é–‹

    def __call__(self, results):
        pts_path = results["pts_filename"]
        scan = np.fromfile(pts_path, dtype=np.float32).reshape((-1, 5))[:, :4]
        scan[:, 3] = 1.0  # (N,4) homogeneous

        if self.use_ego:
            ego2lidar = results["ego2lidar"]          # ego -> lidar
            lidar2ego = np.linalg.inv(ego2lidar)      # lidar -> ego
            scan = (lidar2ego[None, ...] @ scan[..., None]).squeeze(-1)

        scan = scan[:, :3]  # (N,3) meters

        # -------------------------
        # Filter by pc_range (meters)
        # -------------------------
        norm = np.linalg.norm(scan, 2, axis=-1)
        mask = (
            (scan[:, 0] > self.pc_range[0]) & (scan[:, 0] < self.pc_range[3]) &
            (scan[:, 1] > self.pc_range[1]) & (scan[:, 1] < self.pc_range[4]) &
            (scan[:, 2] > self.pc_range[2]) & (scan[:, 2] < self.pc_range[5]) &
            (norm > 1.0)
        )
        scan = scan[mask]

        # å¦‚æœé»æ•¸å¤ªå°‘ï¼Œé¿å…å¾Œé¢ np.random.choice çˆ†æ‰
        if scan.shape[0] == 0:
            # fallbackï¼šå¡ä¸€å€‹åŸé»é™„è¿‘çš„é»ï¼ˆæˆ–ä½ ä¹Ÿå¯ä»¥ raiseï¼‰
            scan = np.zeros((1, 3), dtype=np.float32)

        # -------------------------
        # Sampling to fixed num_pts (still meters)
        # -------------------------
        if scan.shape[0] < self.num_pts:
            choice = np.random.choice(scan.shape[0], self.num_pts, replace=True)
        else:
            choice = np.random.choice(scan.shape[0], self.num_pts, replace=False)
        scan_m = scan[choice].astype(np.float32)  # meters

        # âœ… meters çµ¦ depth æŠ•å½±ç”¨
        results["points_lidar"] = scan_m
        results["anchor_points"] = scan_m  # å»ºè­° anchor_points ä¹Ÿæ”¾ metersï¼Œé¿å…èª¤ç”¨

        # ï¼ˆå¯é¸ï¼‰å¦‚æœä½ çœŸçš„æœ‰æ¨¡çµ„éœ€è¦ 0~1ï¼Œå†å¦å¤–çµ¦ä¸€ä»½
        if self.keep_normalized:
            scan_norm = scan_m.copy()
            scan_norm[:, 0] = (scan_norm[:, 0] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
            scan_norm[:, 1] = (scan_norm[:, 1] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
            scan_norm[:, 2] = (scan_norm[:, 2] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])
            results["anchor_points_norm"] = scan_norm.astype(np.float32)

        return results

    def __repr__(self):
        return self.__class__.__name__



@OPENOCC_TRANSFORMS.register_module()
class LoadPseudoPointFromFile(object):

    def __init__(self, datapath, pc_range, num_pts, is_ego=True, use_ego=False):
        self.datapath = datapath
        self.is_ego = is_ego
        self.use_ego = use_ego
        self.pc_range = pc_range
        self.num_pts = num_pts
        pass

    def __call__(self, results):
        pts_path = os.path.join(self.datapath, f"{results['sample_idx']}.npy")
        scan = np.load(pts_path)
        if self.is_ego and (not self.use_ego):
            ego2lidar = results['ego2lidar']
            scan = np.concatenate([scan, np.ones_like(scan[:, :1])], axis=-1)
            scan = ego2lidar[None, ...] @ scan[..., None] # p, 4, 1
            scan = np.squeeze(scan, axis=-1)

        if (not self.is_ego) and self.use_ego:
            ego2lidar = results['ego2lidar']
            lidar2ego = np.linalg.inv(ego2lidar)
            scan = np.concatenate([scan, np.ones_like(scan[:, :1])], axis=-1)
            scan = lidar2ego[None, ...] @ scan[..., None]
            scan = np.squeeze(scan, axis=-1)
        
        scan = scan[:, :3] # n, 3

        ### filter
        norm = np.linalg.norm(scan, 2, axis=-1)
        mask = (scan[:, 0] > self.pc_range[0]) & (scan[:, 0] < self.pc_range[3]) & \
            (scan[:, 1] > self.pc_range[1]) & (scan[:, 1] < self.pc_range[4]) & \
            (scan[:, 2] > self.pc_range[2]) & (scan[:, 2] < self.pc_range[5]) & \
            (norm > 1.0)
        scan = scan[mask]

        ### append
        if scan.shape[0] < self.num_pts:
            multi = int(math.ceil(self.num_pts * 1.0 / scan.shape[0])) - 1
            scan_ = np.repeat(scan, multi, 0)
            scan_ = scan_ + np.random.randn(*scan_.shape) * 0.3
            scan_ = scan_[np.random.choice(scan_.shape[0], self.num_pts - scan.shape[0], False)]
            scan_[:, 0] = np.clip(scan_[:, 0], self.pc_range[0], self.pc_range[3])
            scan_[:, 1] = np.clip(scan_[:, 1], self.pc_range[1], self.pc_range[4])
            scan_[:, 2] = np.clip(scan_[:, 2], self.pc_range[2], self.pc_range[5])
            scan = np.concatenate([scan, scan_], 0)
        else:
            scan = scan[np.random.choice(scan.shape[0], self.num_pts, False)]
        
        scan[:, 0] = (scan[:, 0] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
        scan[:, 1] = (scan[:, 1] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
        scan[:, 2] = (scan[:, 2] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])
        results['anchor_points'] = scan
        
        return results
    
    def __repr__(self):
        """str: Return a string that describes the module."""
        repr_str = self.__class__.__name__
        return repr_str


@OPENOCC_TRANSFORMS.register_module()
class LoadOccupancySurroundOcc(object):

    def __init__(self, occ_path, semantic=False, use_ego=False, use_sweeps=False, perturb=False):
        self.occ_path = occ_path
        self.semantic = semantic
        self.use_ego = use_ego
        assert semantic and (not use_ego)
        self.use_sweeps = use_sweeps
        self.perturb = perturb

        xyz = self.get_meshgrid([-50, -50, -5.0, 50, 50, 3.0], [200, 200, 16], 0.5)
        self.xyz = np.concatenate([xyz, np.ones_like(xyz[..., :1])], axis=-1) # x, y, z, 4

    def get_meshgrid(self, ranges, grid, reso):
        xxx = torch.arange(grid[0], dtype=torch.float) * reso + 0.5 * reso + ranges[0]
        yyy = torch.arange(grid[1], dtype=torch.float) * reso + 0.5 * reso + ranges[1]
        zzz = torch.arange(grid[2], dtype=torch.float) * reso + 0.5 * reso + ranges[2]

        xxx = xxx[:, None, None].expand(*grid)
        yyy = yyy[None, :, None].expand(*grid)
        zzz = zzz[None, None, :].expand(*grid)

        xyz = torch.stack([
            xxx, yyy, zzz
        ], dim=-1).numpy()
        return xyz # x, y, z, 3

    def __call__(self, results):
        # 1. åŸæœ¬çš„è®€å–é‚è¼¯
        label_file = os.path.join(self.occ_path, results['pts_filename'].split('/')[-1]+'.npy')
        
        # é è¨­å…¨ç©º (17 = empty/free)
        # å‡è¨­ grid size ç‚º [200, 200, 16]
        new_label = np.ones((200, 200, 16), dtype=np.int64) * 17 

        if os.path.exists(label_file):
            label = np.load(label_file)
            # label æ ¼å¼é€šå¸¸æ˜¯ [x, y, z, cls]
            new_label[label[:, 0], label[:, 1], label[:, 2]] = label[:, 3]
        elif self.use_sweeps:
            # å¦‚æœæ˜¯ sweeps æ¨¡å¼ä¸”æ²’æª”æ¡ˆï¼Œä¿æŒå…¨ç©º
            pass
        else:
            raise NotImplementedError

        # âœ… BDA: ç”¨ bda_mat å° new_label åš rot/scale/flipï¼ˆFlashOcc/BEVDet åŒæ–¹å‘ï¼‰
        # =========================================================
        empty_idx = 17
        pc_range = [-50.0, -50.0, -5.0, 50.0, 50.0, 3.0]

        Dx, Dy, Dz = new_label.shape
        vx = (pc_range[3] - pc_range[0]) / Dx  # 0.5
        vy = (pc_range[4] - pc_range[1]) / Dy  # 0.5

        if "bda_mat" in results and results["bda_mat"] is not None:
            bda = results["bda_mat"]

            # bda å¯èƒ½æ˜¯ torch / np
            if hasattr(bda, "detach"):
                bda = bda.detach().cpu().numpy()
            bda = np.asarray(bda, dtype=np.float32)

            # å…è¨± (4,4) -> å– (3,3)
            if bda.shape == (4, 4):
                bda = bda[:3, :3]

            assert bda.shape == (3, 3), f"bda_mat shape should be (3,3), got {bda.shape}"

            # å–å‡ºé empty çš„ voxelï¼ˆempty=17ï¼‰
            idx = np.where(new_label != empty_idx)
            if idx[0].size > 0:
                ix, iy, iz = idx
                cls = new_label[ix, iy, iz]

                # voxel center -> metric (x,y)
                x = pc_range[0] + (ix.astype(np.float32) + 0.5) * vx
                y = pc_range[1] + (iy.astype(np.float32) + 0.5) * vy

                xy1 = np.stack([x, y, np.ones_like(x)], axis=1)  # (M,3)
                xy2 = (xy1 @ bda.T)[:, :2]                      # (M,2)

                ix2 = np.floor((xy2[:, 0] - pc_range[0]) / vx).astype(np.int64)
                iy2 = np.floor((xy2[:, 1] - pc_range[1]) / vy).astype(np.int64)

                inside = (ix2 >= 0) & (ix2 < Dx) & (iy2 >= 0) & (iy2 < Dy)

                out = np.ones_like(new_label, dtype=np.int64) * empty_idx
                out[ix2[inside], iy2[inside], iz[inside]] = cls[inside]
                new_label = out

        # 2. ç”¢ç”Ÿ mask å’Œè¼¸å‡º
        mask = np.ones_like(new_label, dtype=bool)
        results['occ_label'] = new_label if self.semantic else new_label != 17
        results['occ_cam_mask'] = mask

        # 3. è™•ç† occ_xyz (åº§æ¨™ç¶²æ ¼)
        # æ³¨æ„ï¼šå¦‚æœä½ çš„ BDA åªæœ‰ flip/scaleï¼Œocc_xyz é€šå¸¸ä¸éœ€è¦å‹•ï¼Œ
        # å› ç‚º ViewTransformer æœƒæ ¹æ“š bda_mat æŠŠç‰¹å¾µæŠ•å½±åˆ°æ­£ç¢ºçš„ç¿»è½‰ä½ç½®ã€‚
        xyz = self.xyz.copy()
        if getattr(self, "perturb", False):
            norm_distribution = np.clip(np.random.randn(*xyz.shape[:-1], 3) / 6, -0.5, 0.5)
            xyz[..., :3] = xyz[..., :3] + norm_distribution * 0.49

        if not self.use_ego:
            occ_xyz = xyz[..., :3]
        else:
            ego2lidar = np.linalg.inv(results['ego2lidar'])
            occ_xyz = ego2lidar[None, None, None, ...] @ xyz[..., None]
            occ_xyz = np.squeeze(occ_xyz, -1)[..., :3]
            
        results['occ_xyz'] = occ_xyz
        return results

    def __repr__(self):
        """str: Return a string that describes the module."""
        repr_str = self.__class__.__name__
        return repr_str


@OPENOCC_TRANSFORMS.register_module()
class LoadOccupancyKITTI360(object):

    def __init__(self, occ_path, semantic=False, unknown_to_empty=False, training=False):
        self.occ_path = occ_path
        self.semantic = semantic

        xyz = self.get_meshgrid([0.0, -25.6, -2.0, 51.2, 25.6, 4.4], [256, 256, 32], 0.2)
        self.xyz = np.concatenate([xyz, np.ones_like(xyz[..., :1])], axis=-1) # x, y, z, 4
        self.unknown_to_empty = unknown_to_empty
        self.training = training

    def get_meshgrid(self, ranges, grid, reso):
        xxx = torch.arange(grid[0], dtype=torch.float) * reso + 0.5 * reso + ranges[0]
        yyy = torch.arange(grid[1], dtype=torch.float) * reso + 0.5 * reso + ranges[1]
        zzz = torch.arange(grid[2], dtype=torch.float) * reso + 0.5 * reso + ranges[2]

        xxx = xxx[:, None, None].expand(*grid)
        yyy = yyy[None, :, None].expand(*grid)
        zzz = zzz[None, None, :].expand(*grid)

        xyz = torch.stack([
            xxx, yyy, zzz
        ], dim=-1).numpy()
        return xyz # x, y, z, 3

    def __call__(self, results):        
        occ_xyz = self.xyz[..., :3].copy()
        results['occ_xyz'] = occ_xyz

        ## read occupancy label
        label_path = os.path.join(
            self.occ_path, results['sequence'], "{}_1_1.npy".format(results['token']))
        label = np.load(label_path).astype(np.int64)
        if getattr(self, "unknown_to_empty", False) and getattr(self, "training", False):
            label[label == 255] = 0

        results['occ_cam_mask'] = (label != 255)
        results['occ_label'] = label
        return results

    def __repr__(self):
        """str: Return a string that describes the module."""
        repr_str = self.__class__.__name__
        return repr_str

@OPENOCC_TRANSFORMS.register_module()
class LoadTeacherDumpNPZ(object):
    """
    Load teacher dumps:
      bev:  npz -> (K,H,W)   prob/logits ä»»ä¸€ key
      occ3d: npz -> (K,H,W,Z)

    Filename pattern (default):
      {bev_prob_dir}/{split}/{key}{suffix_bev}
      {occ3d_dir}/{split}/{key}{suffix_occ}

    key default:
      key = f"{dump_idx:06d}_0"
    """

    def __init__(
        self,
        bev_prob_dir: str,
        occ3d_dir: str,
        split_key: str = "phase",          # "train"/"val"
        id_key: str = "dump_idx",          # âœ… ç”¨ dump_idx
        suffix_bev: str = "_bev_prob_fp16.npz",
        suffix_occ: str = "_3d_logits_fp16.npz",
        npz_keys_bev=("prob", "logits", "arr_0"),
        npz_keys_occ=("logits", "prob", "arr_0"),
        out_key_bev: str = "teacher_bev_prob",
        out_key_occ: str = "teacher_occ_prob",
        strict: bool = True,               # æ‰¾ä¸åˆ°å°±å ±éŒ¯ï¼ˆå»ºè­°é–‹è‘—ï¼‰
    ):
        self.bev_prob_dir = bev_prob_dir
        self.occ3d_dir = occ3d_dir
        self.split_key = split_key
        self.id_key = id_key
        self.suffix_bev = suffix_bev
        self.suffix_occ = suffix_occ
        self.npz_keys_bev = npz_keys_bev
        self.npz_keys_occ = npz_keys_occ
        self.out_key_bev = out_key_bev
        self.out_key_occ = out_key_occ
        self.strict = strict

    def _make_key(self, v):
        # dump_idx: int -> 000000_0
        if isinstance(v, (int, np.integer)):
            return f"{int(v):06d}_0"
        # è‹¥ä½ çœŸçš„ dump æª”åæ˜¯ tokenï¼Œå°±æœƒèµ°é€™è£¡
        return str(v)

    def _load_npz_anykey(self, path, keys):
        z = np.load(path)
        for k in keys:
            if k in z:
                return z[k]
        # æœ€å¾Œä¿åº•ï¼šå–ç¬¬ä¸€å€‹ array
        if hasattr(z, "files") and len(z.files) > 0:
            return z[z.files[0]]
        raise KeyError(f"NPZ has no expected keys {keys}, files={getattr(z,'files',None)}")

    def __call__(self, results):
        split = results.get(self.split_key, "train")
        sid = results.get(self.id_key, None)
        if sid is None:
            raise KeyError(f"LoadTeacherDumpNPZ: results missing '{self.id_key}'")

        key = self._make_key(sid)

        bev_path = os.path.join(self.bev_prob_dir, split, key + self.suffix_bev)
        occ_path = os.path.join(self.occ3d_dir, split, key + self.suffix_occ)
        if (not os.path.exists(bev_path)) or (not os.path.exists(occ_path)):
            msg = (
                f"[LoadTeacherDumpNPZ] dump not found\n"
                f"  split={split}  id_key={self.id_key}={sid}  key={key}\n"
                f"  bev_path={bev_path}  exists={os.path.exists(bev_path)}\n"
                f"  occ_path={occ_path}  exists={os.path.exists(occ_path)}\n"
                f"  ğŸ‘‰ ä½ ç¾åœ¨ sample_idx æ˜¯ tokenï¼Œä¸æœƒå°åˆ° 000000_0ï¼›è«‹ç”¨ dump_idxã€‚\n"
            )
            if self.strict:
                raise FileNotFoundError(msg)
            else:
                # é strict æ‰å…è¨±å¡ Noneï¼ˆä½†ä½ è¦è‡ªå·±è™•ç† collateï¼‰
                results[self.out_key_bev] = None
                results[self.out_key_occ] = None
                return results

        bev = self._load_npz_anykey(bev_path, self.npz_keys_bev)
        occ = self._load_npz_anykey(occ_path, self.npz_keys_occ)

        if getattr(self, "_dbg", 0) < 5:
            print(f"[DBG][LoadTeacherDumpNPZ] split={split} dump_idx={sid} key={key}")
            print(f"  bev: {bev_path} exists={os.path.exists(bev_path)}")
            print(f"  occ: {occ_path} exists={os.path.exists(occ_path)}")
            self._dbg = getattr(self, "_dbg", 0) + 1

        bev = self._load_npz_anykey(bev_path, self.npz_keys_bev)
        occ = self._load_npz_anykey(occ_path, self.npz_keys_occ)

        # âœ… squeeze dummy leading dim if npz saved as (1,K,H,W) or (1,K,H,W,Z)
        if isinstance(bev, np.ndarray) and bev.ndim == 4 and bev.shape[0] == 1:
            bev = bev[0]  # (K,H,W)

        if isinstance(occ, np.ndarray) and occ.ndim == 5 and occ.shape[0] == 1:
            occ = occ[0]  # (K,H,W,Z)

        results[self.out_key_bev] = np.asarray(bev, dtype=np.float32)
        results[self.out_key_occ] = np.asarray(occ, dtype=np.float32)
        return results


    def __repr__(self):
        return self.__class__.__name__
